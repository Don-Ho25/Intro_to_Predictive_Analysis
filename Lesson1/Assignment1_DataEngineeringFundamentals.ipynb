{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Don-Ho25/Intro_to_Predictive_Analysis/blob/main/Lesson1/Assignment1_DataEngineeringFundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BhovYfSBeZvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "UUy_EBoWZPuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CSV TO PARQUET CONVERSION AND ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"  \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu5D8Sb1ZuBM",
        "outputId": "0d34c27e-60bb-4974-8ea8-cd15bebfc3f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV TO PARQUET CONVERSION AND ANALYSIS\n",
            "============================================================\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create automated dataset with 100+ records\n",
        "print(\"\\nStep 1: Creating automated dataset...\")\n",
        "\n",
        "# Define lists for random selection\n",
        "names = [\n",
        "    \"Amara\", \"Kwame\", \"Zara\", \"Kofi\", \"Asha\",\n",
        "    \"Jabari\", \"Kesi\", \"Tariq\", \"Nia\", \"Sekou\"\n",
        "]\n",
        "\n",
        "cities = [\n",
        "    \"Beijing\", \"Shanghai\", \"Guangzhou\", \"Shenzhen\", \"Chengdu\",\n",
        "    \"Hangzhou\", \"Wuhan\", \"Xi'an\", \"Suzhou\", \"Nanjing\"\n",
        "]\n",
        "\n",
        "# Generate 100 records automatically\n",
        "print(\"Generating 100 fictional user records...\")\n",
        "dataset = []\n",
        "\n",
        "for i in range(100):\n",
        "    record = {\n",
        "        'Name': random.choice(names),\n",
        "        'Age': random.randint(17, 65),\n",
        "        'City': random.choice(cities)\n",
        "    }\n",
        "    dataset.append(record)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(f\"Dataset created with {len(df)} records\")\n",
        "print(\"\\nFirst 10 records:\")\n",
        "print(df.head(10))\n",
        "\n",
        "print(f\"\\nDataset statistics:\")\n",
        "print(f\"- Total records: {len(df)}\")\n",
        "print(f\"- Unique names: {df['Name'].nunique()}\")\n",
        "print(f\"- Unique cities: {df['City'].nunique()}\")\n",
        "print(f\"- Age range: {df['Age'].min()} - {df['Age'].max()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhSSxhNKZl6g",
        "outputId": "734f73a9-021d-4d7c-89ce-ebf771bae394"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Creating automated dataset...\n",
            "Generating 100 fictional user records...\n",
            "Dataset created with 100 records\n",
            "\n",
            "First 10 records:\n",
            "    Name  Age      City\n",
            "0  Amara   64  Hangzhou\n",
            "1  Amara   20   Nanjing\n",
            "2  Tariq   49    Suzhou\n",
            "3   Zara   20    Suzhou\n",
            "4  Kwame   28  Shanghai\n",
            "5  Sekou   21  Shenzhen\n",
            "6   Kesi   24   Nanjing\n",
            "7   Kofi   54   Nanjing\n",
            "8  Amara   56  Shanghai\n",
            "9   Kesi   59   Nanjing\n",
            "\n",
            "Dataset statistics:\n",
            "- Total records: 100\n",
            "- Unique names: 10\n",
            "- Unique cities: 10\n",
            "- Age range: 17 - 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Save as CSV and convert to Parquet\n",
        "print(\"\\nStep 2: Converting CSV to Parquet...\")\n",
        "\n",
        "csv_filename = 'user_data.csv'\n",
        "parquet_filename = 'user_data.parquet'\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"✓ CSV file saved: {csv_filename}\")\n",
        "\n",
        "# Convert to Parquet\n",
        "df.to_parquet(parquet_filename, index=False)\n",
        "print(f\"✓ Parquet file saved: {parquet_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ic_AxTWZ1zU",
        "outputId": "5f006919-ab27-4bbd-ce86-386c2698df36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Converting CSV to Parquet...\n",
            "✓ CSV file saved: user_data.csv\n",
            "✓ Parquet file saved: user_data.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Compare file sizes\n",
        "print(\"\\nStep 3: File size comparison...\")\n",
        "\n",
        "csv_size = os.path.getsize(csv_filename)\n",
        "parquet_size = os.path.getsize(parquet_filename)\n",
        "\n",
        "print(f\"CSV file size: {csv_size:,} bytes ({csv_size/1024:.2f} KB)\")\n",
        "print(f\"Parquet file size: {parquet_size:,} bytes ({parquet_size/1024:.2f} KB)\")\n",
        "\n",
        "size_difference = csv_size - parquet_size\n",
        "compression_ratio = (size_difference / csv_size) * 100\n",
        "\n",
        "print(f\"Size difference: {size_difference:,} bytes\")\n",
        "print(f\"Compression ratio: {compression_ratio:.1f}% smaller\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYh5G4OsZ9iG",
        "outputId": "ddb4db92-9fb1-47a4-fd9b-25215a946075"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: File size comparison...\n",
            "CSV file size: 1,647 bytes (1.61 KB)\n",
            "Parquet file size: 2,612 bytes (2.55 KB)\n",
            "Size difference: -965 bytes\n",
            "Compression ratio: -58.6% smaller\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis**\n",
        "\n",
        "Contrary to typical expectations where Parquet files often offer significant compression advantages over CSV, in this specific instance, the Parquet file is actually larger than the CSV file by 965 bytes. This results in a negative \"compression ratio\" of −58.6%, indicating that the Parquet file is approximately 58.6% larger than the CSV file."
      ],
      "metadata": {
        "id": "jAFSH83raZKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load Parquet file and verify data integrity\n",
        "print(\"\\nStep 4: Loading Parquet file and verifying data integrity...\")\n",
        "\n",
        "df_parquet = pd.read_parquet(parquet_filename)\n",
        "\n",
        "# Verify data integrity\n",
        "print(f\"Original DataFrame shape: {df.shape}\")\n",
        "print(f\"Parquet DataFrame shape: {df_parquet.shape}\")\n",
        "\n",
        "# Check if data is identical\n",
        "data_identical = df.equals(df_parquet)\n",
        "print(f\"Data integrity check: {' PASSED' if data_identical else ' FAILED'}\")\n",
        "\n",
        "if data_identical:\n",
        "    print(\"All data successfully preserved during conversion!\")\n",
        "else:\n",
        "    print(\"WARNING: Data differences detected!\")\n",
        "\n",
        "# Additional verification\n",
        "print(\"\\nDetailed verification:\")\n",
        "print(f\"- Row count match: {len(df) == len(df_parquet)}\")\n",
        "print(f\"- Column count match: {len(df.columns) == len(df_parquet.columns)}\")\n",
        "print(f\"- Column names match: {list(df.columns) == list(df_parquet.columns)}\")\n",
        "\n",
        "# Sample comparison\n",
        "print(\"\\nSample data comparison (first 5 rows):\")\n",
        "print(\"ORIGINAL CSV DATA:\")\n",
        "print(df.head())\n",
        "print(\"\\nPARQUET DATA:\")\n",
        "print(df_parquet.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8IvtzOOaifr",
        "outputId": "0d20d8d9-8395-4ab5-dbc0-833c07402d80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Loading Parquet file and verifying data integrity...\n",
            "Original DataFrame shape: (100, 3)\n",
            "Parquet DataFrame shape: (100, 3)\n",
            "Data integrity check:  PASSED\n",
            "All data successfully preserved during conversion!\n",
            "\n",
            "Detailed verification:\n",
            "- Row count match: True\n",
            "- Column count match: True\n",
            "- Column names match: True\n",
            "\n",
            "Sample data comparison (first 5 rows):\n",
            "ORIGINAL CSV DATA:\n",
            "    Name  Age      City\n",
            "0  Amara   64  Hangzhou\n",
            "1  Amara   20   Nanjing\n",
            "2  Tariq   49    Suzhou\n",
            "3   Zara   20    Suzhou\n",
            "4  Kwame   28  Shanghai\n",
            "\n",
            "PARQUET DATA:\n",
            "    Name  Age      City\n",
            "0  Amara   64  Hangzhou\n",
            "1  Amara   20   Nanjing\n",
            "2  Tariq   49    Suzhou\n",
            "3   Zara   20    Suzhou\n",
            "4  Kwame   28  Shanghai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLekN0D7aicU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create comprehensive report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPREHENSIVE ANALYSIS REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\"\"\n",
        "DATASET SUMMARY:\n",
        "- Records generated: {len(df)}\n",
        "- Data generation method: Automated using random.choice()\n",
        "- Name pool: {len(names)} unique names\n",
        "- City pool: {len(cities)} unique cities\n",
        "- Age range: {df['Age'].min()} to {df['Age'].max()} years\n",
        "\n",
        "FILE FORMAT COMPARISON:\n",
        "- CSV file size: {csv_size:,} bytes ({csv_size/1024:.2f} KB)\n",
        "- Parquet file size: {parquet_size:,} bytes ({parquet_size/1024:.2f} KB)\n",
        "- Size reduction: {abs(size_difference):,} bytes ({abs(compression_ratio):.1f}%)\n",
        "- Parquet is {'smaller' if parquet_size < csv_size else 'larger'} than CSV\n",
        "\n",
        "DATA INTEGRITY:\n",
        "- Data preservation: {'✓ Complete' if data_identical else '✗ Issues detected'}\n",
        "- All records intact: {len(df) == len(df_parquet)}\n",
        "- Schema preserved: {list(df.columns) == list(df_parquet.columns)}\n",
        "\"\"\")\n",
        "\n",
        "print(\"OBSERVATIONS:\")\n",
        "observations = []\n",
        "\n",
        "if parquet_size < csv_size:\n",
        "    observations.append(f\"• Parquet format achieved {compression_ratio:.1f}% size reduction\")\n",
        "    observations.append(\"• Parquet's columnar storage is more efficient for this dataset\")\n",
        "else:\n",
        "    observations.append(\"• CSV format is smaller for this particular dataset\")\n",
        "    observations.append(\"• Small datasets may not benefit from Parquet compression\")\n",
        "\n",
        "observations.append(f\"• Dataset contains {df['Name'].nunique()} unique names across {len(df)} records\")\n",
        "observations.append(f\"• Name distribution: {dict(df['Name'].value_counts().head(3))}\")\n",
        "observations.append(f\"• City distribution: {dict(df['City'].value_counts().head(3))}\")\n",
        "observations.append(f\"• Average age: {df['Age'].mean():.1f} years\")\n",
        "\n",
        "for obs in observations:\n",
        "    print(obs)\n",
        "\n",
        "print(f\"\\nRECOMMENDations:\")\n",
        "print(\"• Parquet format recommended for:\")\n",
        "print(\"  - Large datasets (>1000 records)\")\n",
        "print(\"  - Analytical workloads\")\n",
        "print(\"  - Long-term storage\")\n",
        "print(\"  - When file size matters\")\n",
        "print(\"• CSV format recommended for:\")\n",
        "print(\"  - Small datasets\")\n",
        "print(\"  - Human readability\")\n",
        "print(\"  - Simple data exchange\")\n",
        "print(\"  - When compatibility is crucial\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQCqhguAYkkj",
        "outputId": "f11ab5ab-3da2-4b20-a6c2-e82babda7bc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COMPREHENSIVE ANALYSIS REPORT\n",
            "============================================================\n",
            "\n",
            "DATASET SUMMARY:\n",
            "- Records generated: 100\n",
            "- Data generation method: Automated using random.choice()\n",
            "- Name pool: 10 unique names\n",
            "- City pool: 10 unique cities\n",
            "- Age range: 17 to 64 years\n",
            "\n",
            "FILE FORMAT COMPARISON:\n",
            "- CSV file size: 1,647 bytes (1.61 KB)\n",
            "- Parquet file size: 2,612 bytes (2.55 KB)\n",
            "- Size reduction: 965 bytes (58.6%)\n",
            "- Parquet is larger than CSV\n",
            "\n",
            "DATA INTEGRITY:\n",
            "- Data preservation: ✓ Complete\n",
            "- All records intact: True\n",
            "- Schema preserved: True\n",
            "\n",
            "OBSERVATIONS:\n",
            "• CSV format is smaller for this particular dataset\n",
            "• Small datasets may not benefit from Parquet compression\n",
            "• Dataset contains 10 unique names across 100 records\n",
            "• Name distribution: {'Asha': np.int64(15), 'Zara': np.int64(12), 'Kofi': np.int64(12)}\n",
            "• City distribution: {'Suzhou': np.int64(14), 'Hangzhou': np.int64(12), 'Shanghai': np.int64(12)}\n",
            "• Average age: 40.3 years\n",
            "\n",
            "RECOMMENDations:\n",
            "• Parquet format recommended for:\n",
            "  - Large datasets (>1000 records)\n",
            "  - Analytical workloads\n",
            "  - Long-term storage\n",
            "  - When file size matters\n",
            "• CSV format recommended for:\n",
            "  - Small datasets\n",
            "  - Human readability\n",
            "  - Simple data exchange\n",
            "  - When compatibility is crucial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASIC ETL PIPELINE IMPLEMENTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# First, let's create sample data for demonstration\n",
        "print(\"Creating sample dataset for ETL demonstration...\")\n",
        "\n",
        "# Create diverse sample data with some under-18 records for filtering\n",
        "sample_data = {\n",
        "    'Name': [\n",
        "        'Amara Johnson', 'Kwame Smith', 'Zara Brown', 'Kofi Davis', 'Asha Wilson',\n",
        "        'Jabari Miller', 'Kesi Jones', 'Tariq Garcia', 'Nia Rodriguez', 'Sekou Martinez',\n",
        "        'Fatima Lopez', 'Chinonso Hernandez', 'Adunni Anderson', 'Olumide Taylor',\n",
        "        'Chioma Thomas', 'Emeka Jackson', 'Ngozi White', 'Adebayo Harris',\n",
        "        'Funmi Martin', 'Tunde Thompson'\n",
        "    ],\n",
        "    'Age': [\n",
        "        25, 17, 30, 16, 28, 35, 15, 22, 31, 19,\n",
        "        24, 18, 27, 14, 33, 29, 20, 26, 32, 21\n",
        "    ],\n",
        "    'City': [\n",
        "        'Beijing', 'Shanghai', 'Guangzhou', 'Shenzhen', 'Chengdu',\n",
        "        'Hangzhou', 'Wuhan', 'Xi\\'an', 'Suzhou', 'Nanjing',\n",
        "        'Beijing', 'Shanghai', 'Guangzhou', 'Shenzhen', 'Chengdu',\n",
        "        'Hangzhou', 'Wuhan', 'Xi\\'an', 'Suzhou', 'Nanjing'\n",
        "    ],\n",
        "    'Salary': [\n",
        "        50000, 35000, 75000, 28000, 65000, 85000, 25000, 45000, 72000, 42000,\n",
        "        55000, 38000, 68000, 22000, 90000, 78000, 48000, 62000, 80000, 52000\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create DataFrame and save as source CSV\n",
        "source_df = pd.DataFrame(sample_data)\n",
        "source_file = 'source_data.csv'\n",
        "source_df.to_csv(source_file, index=False)\n",
        "\n",
        "print(f\"Sample dataset created with {len(source_df)} records\")\n",
        "print(f\"Source file saved: {source_file}\")\n",
        "\n",
        "# ETL PIPELINE IMPLEMENTATION\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING ETL PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# STEP 1: EXTRACT DATA FROM CSV FILE\n",
        "print(\"\\nSTEP 1: EXTRACT - Loading data from CSV file\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Load data from CSV file\n",
        "    extracted_df = pd.read_csv(source_file)\n",
        "    print(f\"✓ Successfully extracted {len(extracted_df)} records from {source_file}\")\n",
        "    print(f\"✓ Columns: {list(extracted_df.columns)}\")\n",
        "\n",
        "    print(\"\\nExtracted data preview:\")\n",
        "    print(extracted_df.head(10))\n",
        "\n",
        "    # Data quality check\n",
        "    print(f\"\\nData quality summary:\")\n",
        "    print(f\"- Total records: {len(extracted_df)}\")\n",
        "    print(f\"- Records with age < 18: {len(extracted_df[extracted_df['Age'] < 18])}\")\n",
        "    print(f\"- Records with age >= 18: {len(extracted_df[extracted_df['Age'] >= 18])}\")\n",
        "    print(f\"- Missing values: {extracted_df.isnull().sum().sum()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during extraction: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# STEP 2: TRANSFORM DATA\n",
        "print(\"\\nSTEP 2: TRANSFORM - Filtering and converting data\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Create a copy for transformation\n",
        "    transformed_df = extracted_df.copy()\n",
        "\n",
        "    # Show original data statistics\n",
        "    print(f\"Before transformation:\")\n",
        "    print(f\"- Total records: {len(transformed_df)}\")\n",
        "    print(f\"- Records with age < 18: {len(transformed_df[transformed_df['Age'] < 18])}\")\n",
        "    print(f\"- Sample names: {transformed_df['Name'].head(3).tolist()}\")\n",
        "\n",
        "    # Transformation 1: Filter out records where age is below 18\n",
        "    print(\"\\nApplying transformation 1: Filtering age >= 18...\")\n",
        "    records_before_filter = len(transformed_df)\n",
        "    transformed_df = transformed_df[transformed_df['Age'] >= 18]\n",
        "    records_after_filter = len(transformed_df)\n",
        "    records_filtered = records_before_filter - records_after_filter\n",
        "\n",
        "    print(f\"✓ Filtered out {records_filtered} records with age < 18\")\n",
        "    print(f\"✓ Remaining records: {records_after_filter}\")\n",
        "\n",
        "    # Transformation 2: Convert names to uppercase\n",
        "    print(\"\\nApplying transformation 2: Converting names to uppercase...\")\n",
        "    transformed_df['Name'] = transformed_df['Name'].str.upper()\n",
        "    print(f\"✓ All names converted to uppercase\")\n",
        "    print(f\"✓ Sample transformed names: {transformed_df['Name'].head(3).tolist()}\")\n",
        "\n",
        "    print(f\"\\nAfter transformation:\")\n",
        "    print(f\"- Total records: {len(transformed_df)}\")\n",
        "    print(f\"- All ages >= 18: {all(transformed_df['Age'] >= 18)}\")\n",
        "    print(f\"- All names uppercase: {all(name.isupper() for name in transformed_df['Name'])}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during transformation: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# STEP 3: LOAD TRANSFORMED DATA INTO NEW CSV FILE\n",
        "print(\"\\nSTEP 3: LOAD - Saving transformed data to new CSV file\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Save transformed data to new CSV file\n",
        "    output_file = 'transformed_data.csv'\n",
        "    transformed_df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"✓ Transformed data saved to {output_file}\")\n",
        "    print(f\"✓ File contains {len(transformed_df)} records\")\n",
        "\n",
        "    # Check file was created successfully\n",
        "    import os\n",
        "    if os.path.exists(output_file):\n",
        "        file_size = os.path.getsize(output_file)\n",
        "        print(f\"✓ File size: {file_size} bytes\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during loading: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# STEP 4: VERIFY TRANSFORMATION\n",
        "print(\"\\nSTEP 4: VERIFY - Loading and displaying the new file\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    # Load the transformed file to verify\n",
        "    verification_df = pd.read_csv(output_file)\n",
        "\n",
        "    print(f\"✓ Successfully loaded {len(verification_df)} records from {output_file}\")\n",
        "\n",
        "    # Verification checks\n",
        "    print(\"\\nVerification Results:\")\n",
        "    print(f\"- Records in transformed file: {len(verification_df)}\")\n",
        "    print(f\"- All ages >= 18: {all(verification_df['Age'] >= 18)}\")\n",
        "    print(f\"- All names uppercase: {all(name.isupper() for name in verification_df['Name'])}\")\n",
        "    print(f\"- Columns preserved: {list(verification_df.columns) == list(extracted_df.columns)}\")\n",
        "\n",
        "    # Display transformed data\n",
        "    print(\"\\nTransformed data (first 10 records):\")\n",
        "    print(verification_df.head(10))\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\nSummary statistics:\")\n",
        "    print(f\"- Age range: {verification_df['Age'].min()} - {verification_df['Age'].max()}\")\n",
        "    print(f\"- Average age: {verification_df['Age'].mean():.1f}\")\n",
        "    print(f\"- Unique cities: {verification_df['City'].nunique()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during verification: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# STEP 5: FINAL SUMMARY AND FILE CONFIRMATION\n",
        "print(\"\\nSTEP 5: FINAL SUMMARY - ETL Pipeline Complete\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(f\"\"\"\n",
        "ETL PIPELINE EXECUTION SUMMARY:\n",
        "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "✓ EXTRACT: Loaded {len(extracted_df)} records from {source_file}\n",
        "✓ TRANSFORM:\n",
        "  - Filtered {records_filtered} records with age < 18\n",
        "  - Converted all names to uppercase\n",
        "  - Final dataset: {len(transformed_df)} records\n",
        "✓ LOAD: Saved transformed data to {output_file}\n",
        "✓ VERIFY: Confirmed data integrity and transformations\n",
        "\n",
        "FILES CREATED:\n",
        "- Source file: {source_file} ({len(source_df)} records)\n",
        "- Transformed file: {output_file} ({len(verification_df)} records)\n",
        "\n",
        "TRANSFORMATION SUMMARY:\n",
        "- Records removed (age < 18): {records_filtered}\n",
        "- Records retained: {len(verification_df)}\n",
        "- Names converted to uppercase: {len(verification_df)}\n",
        "- Data integrity: ✓ VERIFIED\n",
        "\n",
        "The ETL pipeline has been successfully completed!\n",
        "\"\"\")\n",
        "\n",
        "# Display side-by-side comparison\n",
        "print(\"\\nBEFORE vs AFTER COMPARISON:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"ORIGINAL DATA (first 5 records):\")\n",
        "print(extracted_df.head())\n",
        "print(\"\\nTRANSFORMED DATA (first 5 records):\")\n",
        "print(verification_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdz2uj3lYlU3",
        "outputId": "35796454-639d-488c-984d-3bbd9d544854"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASIC ETL PIPELINE IMPLEMENTATION\n",
            "============================================================\n",
            "Creating sample dataset for ETL demonstration...\n",
            "Sample dataset created with 20 records\n",
            "Source file saved: source_data.csv\n",
            "\n",
            "============================================================\n",
            "STARTING ETL PIPELINE\n",
            "============================================================\n",
            "\n",
            "STEP 1: EXTRACT - Loading data from CSV file\n",
            "----------------------------------------\n",
            "✓ Successfully extracted 20 records from source_data.csv\n",
            "✓ Columns: ['Name', 'Age', 'City', 'Salary']\n",
            "\n",
            "Extracted data preview:\n",
            "             Name  Age       City  Salary\n",
            "0   Amara Johnson   25    Beijing   50000\n",
            "1     Kwame Smith   17   Shanghai   35000\n",
            "2      Zara Brown   30  Guangzhou   75000\n",
            "3      Kofi Davis   16   Shenzhen   28000\n",
            "4     Asha Wilson   28    Chengdu   65000\n",
            "5   Jabari Miller   35   Hangzhou   85000\n",
            "6      Kesi Jones   15      Wuhan   25000\n",
            "7    Tariq Garcia   22      Xi'an   45000\n",
            "8   Nia Rodriguez   31     Suzhou   72000\n",
            "9  Sekou Martinez   19    Nanjing   42000\n",
            "\n",
            "Data quality summary:\n",
            "- Total records: 20\n",
            "- Records with age < 18: 4\n",
            "- Records with age >= 18: 16\n",
            "- Missing values: 0\n",
            "\n",
            "STEP 2: TRANSFORM - Filtering and converting data\n",
            "----------------------------------------\n",
            "Before transformation:\n",
            "- Total records: 20\n",
            "- Records with age < 18: 4\n",
            "- Sample names: ['Amara Johnson', 'Kwame Smith', 'Zara Brown']\n",
            "\n",
            "Applying transformation 1: Filtering age >= 18...\n",
            "✓ Filtered out 4 records with age < 18\n",
            "✓ Remaining records: 16\n",
            "\n",
            "Applying transformation 2: Converting names to uppercase...\n",
            "✓ All names converted to uppercase\n",
            "✓ Sample transformed names: ['AMARA JOHNSON', 'ZARA BROWN', 'ASHA WILSON']\n",
            "\n",
            "After transformation:\n",
            "- Total records: 16\n",
            "- All ages >= 18: True\n",
            "- All names uppercase: True\n",
            "\n",
            "STEP 3: LOAD - Saving transformed data to new CSV file\n",
            "----------------------------------------\n",
            "✓ Transformed data saved to transformed_data.csv\n",
            "✓ File contains 16 records\n",
            "✓ File size: 516 bytes\n",
            "\n",
            "STEP 4: VERIFY - Loading and displaying the new file\n",
            "----------------------------------------\n",
            "✓ Successfully loaded 16 records from transformed_data.csv\n",
            "\n",
            "Verification Results:\n",
            "- Records in transformed file: 16\n",
            "- All ages >= 18: True\n",
            "- All names uppercase: True\n",
            "- Columns preserved: True\n",
            "\n",
            "Transformed data (first 10 records):\n",
            "                 Name  Age       City  Salary\n",
            "0       AMARA JOHNSON   25    Beijing   50000\n",
            "1          ZARA BROWN   30  Guangzhou   75000\n",
            "2         ASHA WILSON   28    Chengdu   65000\n",
            "3       JABARI MILLER   35   Hangzhou   85000\n",
            "4        TARIQ GARCIA   22      Xi'an   45000\n",
            "5       NIA RODRIGUEZ   31     Suzhou   72000\n",
            "6      SEKOU MARTINEZ   19    Nanjing   42000\n",
            "7        FATIMA LOPEZ   24    Beijing   55000\n",
            "8  CHINONSO HERNANDEZ   18   Shanghai   38000\n",
            "9     ADUNNI ANDERSON   27  Guangzhou   68000\n",
            "\n",
            "Summary statistics:\n",
            "- Age range: 18 - 35\n",
            "- Average age: 26.2\n",
            "- Unique cities: 9\n",
            "\n",
            "STEP 5: FINAL SUMMARY - ETL Pipeline Complete\n",
            "----------------------------------------\n",
            "\n",
            "ETL PIPELINE EXECUTION SUMMARY:\n",
            "2025-07-12 20:58:01\n",
            "\n",
            "✓ EXTRACT: Loaded 20 records from source_data.csv\n",
            "✓ TRANSFORM: \n",
            "  - Filtered 4 records with age < 18\n",
            "  - Converted all names to uppercase\n",
            "  - Final dataset: 16 records\n",
            "✓ LOAD: Saved transformed data to transformed_data.csv\n",
            "✓ VERIFY: Confirmed data integrity and transformations\n",
            "\n",
            "FILES CREATED:\n",
            "- Source file: source_data.csv (20 records)\n",
            "- Transformed file: transformed_data.csv (16 records)\n",
            "\n",
            "TRANSFORMATION SUMMARY:\n",
            "- Records removed (age < 18): 4\n",
            "- Records retained: 16\n",
            "- Names converted to uppercase: 16\n",
            "- Data integrity: ✓ VERIFIED\n",
            "\n",
            "The ETL pipeline has been successfully completed!\n",
            "\n",
            "\n",
            "BEFORE vs AFTER COMPARISON:\n",
            "============================================================\n",
            "ORIGINAL DATA (first 5 records):\n",
            "            Name  Age       City  Salary\n",
            "0  Amara Johnson   25    Beijing   50000\n",
            "1    Kwame Smith   17   Shanghai   35000\n",
            "2     Zara Brown   30  Guangzhou   75000\n",
            "3     Kofi Davis   16   Shenzhen   28000\n",
            "4    Asha Wilson   28    Chengdu   65000\n",
            "\n",
            "TRANSFORMED DATA (first 5 records):\n",
            "            Name  Age       City  Salary\n",
            "0  AMARA JOHNSON   25    Beijing   50000\n",
            "1     ZARA BROWN   30  Guangzhou   75000\n",
            "2    ASHA WILSON   28    Chengdu   65000\n",
            "3  JABARI MILLER   35   Hangzhou   85000\n",
            "4   TARIQ GARCIA   22      Xi'an   45000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n"
      ],
      "metadata": {
        "id": "yUJbsZcqc0rS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SQLITE vs MONGODB COMPARISON\")\n",
        "print(\"=\" * 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZFuicQQc53U",
        "outputId": "633af7ab-7e5a-40d1-a217-53a8f5afe5f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQLITE vs MONGODB COMPARISON\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for both databases\n",
        "sample_users = [\n",
        "    {\"ID\": 1, \"Name\": \"Amara Johnson\", \"Age\": 25, \"City\": \"Beijing\", \"Department\": \"Engineering\", \"Salary\": 75000},\n",
        "    {\"ID\": 2, \"Name\": \"Kwame Smith\", \"Age\": 30, \"City\": \"Shanghai\", \"Department\": \"Marketing\", \"Salary\": 65000},\n",
        "    {\"ID\": 3, \"Name\": \"Zara Brown\", \"Age\": 28, \"City\": \"Guangzhou\", \"Department\": \"Finance\", \"Salary\": 70000},\n",
        "    {\"ID\": 4, \"Name\": \"Kofi Davis\", \"Age\": 32, \"City\": \"Shenzhen\", \"Department\": \"HR\", \"Salary\": 60000},\n",
        "    {\"ID\": 5, \"Name\": \"Asha Wilson\", \"Age\": 27, \"City\": \"Chengdu\", \"Department\": \"Engineering\", \"Salary\": 78000},\n",
        "    {\"ID\": 6, \"Name\": \"Jabari Miller\", \"Age\": 35, \"City\": \"Hangzhou\", \"Department\": \"Sales\", \"Salary\": 72000}\n",
        "]\n",
        "\n",
        "# Additional flexible data for MongoDB (showing document model flexibility)\n",
        "flexible_users = [\n",
        "    {\n",
        "        \"ID\": 7, \"Name\": \"Fatima Chen\", \"Age\": 29, \"City\": \"Wuhan\",\n",
        "        \"Department\": \"Engineering\", \"Salary\": 80000,\n",
        "        \"Skills\": [\"Python\", \"JavaScript\", \"React\"],\n",
        "        \"Projects\": [\n",
        "            {\"name\": \"E-commerce Platform\", \"status\": \"completed\"},\n",
        "            {\"name\": \"Mobile App\", \"status\": \"in_progress\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"ID\": 8, \"Name\": \"Chinonso Wang\", \"Age\": 31, \"City\": \"Xi'an\",\n",
        "        \"Department\": \"Marketing\", \"Salary\": 68000,\n",
        "        \"Skills\": [\"SEO\", \"Content Marketing\"],\n",
        "        \"Certifications\": [\"Google Analytics\", \"Facebook Ads\"]\n",
        "    }\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "1KQ7SQsvdGBQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PART 1: SQLite RELATIONAL DATABASE OPERATIONS\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKByVsqydJUk",
        "outputId": "82ee3ffa-80db-40d5-af66-c5967461e2e5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PART 1: SQLite RELATIONAL DATABASE OPERATIONS\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sqlite_operations():\n",
        "    \"\"\"Demonstrate SQLite relational database operations\"\"\"\n",
        "\n",
        "    # Create/connect to SQLite database\n",
        "    db_file = 'users.db'\n",
        "    print(f\"\\n1. Creating SQLite database: {db_file}\")\n",
        "\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create table\n",
        "        print(\"2. Creating users table...\")\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS users (\n",
        "                ID INTEGER PRIMARY KEY,\n",
        "                Name TEXT NOT NULL,\n",
        "                Age INTEGER NOT NULL,\n",
        "                City TEXT NOT NULL,\n",
        "                Department TEXT,\n",
        "                Salary REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        # Clear existing data for fresh start\n",
        "        cursor.execute(\"DELETE FROM users\")\n",
        "\n",
        "        # Insert sample data\n",
        "        print(\"3. Inserting sample records...\")\n",
        "        for user in sample_users:\n",
        "            cursor.execute('''\n",
        "                INSERT INTO users (ID, Name, Age, City, Department, Salary)\n",
        "                VALUES (?, ?, ?, ?, ?, ?)\n",
        "            ''', (user['ID'], user['Name'], user['Age'], user['City'],\n",
        "                  user['Department'], user['Salary']))\n",
        "\n",
        "        conn.commit()\n",
        "        print(f\"✓ Inserted {len(sample_users)} records into SQLite\")\n",
        "\n",
        "        # Retrieve and display data\n",
        "        print(\"\\n4. Retrieving data using SQL queries:\")\n",
        "\n",
        "        # Query 1: All users\n",
        "        print(\"\\nQuery 1: All users\")\n",
        "        cursor.execute(\"SELECT * FROM users\")\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        print(f\"{'ID':<3} {'Name':<15} {'Age':<4} {'City':<12} {'Department':<12} {'Salary':<8}\")\n",
        "        print(\"-\" * 65)\n",
        "        for row in results:\n",
        "            print(f\"{row[0]:<3} {row[1]:<15} {row[2]:<4} {row[3]:<12} {row[4]:<12} {row[5]:<8}\")\n",
        "\n",
        "        # Query 2: Users older than 29\n",
        "        print(\"\\nQuery 2: Users older than 29\")\n",
        "        cursor.execute(\"SELECT * FROM users WHERE Age > 29\")\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        print(\"\\nUsers older than 29:\")\n",
        "        print(f\"{'ID':<3} {'Name':<15} {'Age':<4} {'City':<12} {'Department':<12} {'Salary':<8}\")\n",
        "        print(\"-\" * 65)\n",
        "        for row in results:\n",
        "            print(f\"{row[0]:<3} {row[1]:<15} {row[2]:<4} {row[3]:<12} {row[4]:<12} {row[5]:<8}\")\n",
        "\n",
        "        # Query 3: Average salary by department\n",
        "        print(\"\\nQuery 3: Average salary by department\")\n",
        "        cursor.execute(\"SELECT Department, AVG(Salary) FROM users GROUP BY Department\")\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        print(\"\\nAverage salary by department:\")\n",
        "        print(f\"{'Department':<15} {'Average Salary':<15}\")\n",
        "        print(\"-\" * 30)\n",
        "        for row in results:\n",
        "            print(f\"{row[0]:<15} {row[1]:<15.2f}\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"✗ SQLite error: {e}\")\n",
        "    finally:\n",
        "        if 'conn' in locals() and conn:\n",
        "            conn.close()\n",
        "            print(f\"\\nSQLite connection closed.\")\n"
      ],
      "metadata": {
        "id": "JraheFaqdTZW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PART 2: MongoDB DOCUMENT DATABASE OPERATIONS\")\n",
        "print(\"=\" * 70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H62Wcev1dYYj",
        "outputId": "438ffcee-46bc-4deb-a72c-ce685ddb8ef9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PART 2: MongoDB DOCUMENT DATABASE OPERATIONS\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mongodb_operations():\n",
        "    \"\"\"Demonstrate MongoDB document database operations\"\"\"\n",
        "\n",
        "    if not MONGODB_AVAILABLE:\n",
        "        print(\"MongoDB operations skipped: PyMongo not installed.\")\n",
        "        return\n",
        "\n",
        "    # Connect to MongoDB\n",
        "    try:\n",
        "        print(\"\\n1. Connecting to MongoDB...\")\n",
        "        # Replace 'mongodb://localhost:27017/' with your MongoDB connection string if needed\n",
        "        client = MongoClient('mongodb://localhost:27017/', serverSelectionTimeoutMS=5000)\n",
        "        client.admin.command('ping') # Check connection\n",
        "        print(\"✓ Connected to MongoDB!\")\n",
        "\n",
        "        db = client['user_database'] # Use a database named 'user_database'\n",
        "        collection = db['users'] # Use a collection named 'users'\n",
        "\n",
        "        # Clear existing data for fresh start\n",
        "        print(\"2. Clearing existing records in MongoDB collection...\")\n",
        "        delete_result = collection.delete_many({})\n",
        "        print(f\"✓ Deleted {delete_result.deleted_count} existing records\")\n",
        "\n",
        "        # Insert sample data (including flexible data)\n",
        "        print(\"3. Inserting sample records...\")\n",
        "        all_users_data = sample_users + flexible_users\n",
        "        insert_result = collection.insert_many(all_users_data)\n",
        "        print(f\"✓ Inserted {len(insert_result.inserted_ids)} records into MongoDB\")\n",
        "\n",
        "        # Retrieve and display data\n",
        "        print(\"\\n4. Retrieving data using MongoDB queries:\")\n",
        "\n",
        "        # Query 1: All users\n",
        "        print(\"\\nQuery 1: All users\")\n",
        "        results = list(collection.find({}).limit(10)) # Limit to 10 for preview\n",
        "\n",
        "        print(\"\\nAll users (first 10):\")\n",
        "        for user in results:\n",
        "            # Print relevant fields, handling flexible schema\n",
        "            print(f\"ID: {user.get('ID')}, Name: {user.get('Name')}, Age: {user.get('Age')}, City: {user.get('City')}, Department: {user.get('Department')}, Salary: {user.get('Salary')}\")\n",
        "            if 'Skills' in user:\n",
        "                print(f\"  Skills: {user['Skills']}\")\n",
        "            if 'Projects' in user:\n",
        "                print(f\"  Projects: {user['Projects']}\")\n",
        "            if 'Certifications' in user:\n",
        "                print(f\"  Certifications: {user['Certifications']}\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "        # Query 2: Users older than 29\n",
        "        print(\"\\nQuery 2: Users older than 29\")\n",
        "        results = list(collection.find({\"Age\": {\"$gt\": 29}}))\n",
        "\n",
        "        print(\"\\nUsers older than 29:\")\n",
        "        for user in results:\n",
        "             print(f\"ID: {user.get('ID')}, Name: {user.get('Name')}, Age: {user.get('Age')}, City: {user.get('City')}\")\n",
        "\n",
        "        # Query 3: Users with 'Engineering' department\n",
        "        print(\"\\nQuery 3: Users in Engineering department\")\n",
        "        results = list(collection.find({\"Department\": \"Engineering\"}))\n",
        "\n",
        "        print(\"\\nUsers in Engineering department:\")\n",
        "        for user in results:\n",
        "             print(f\"ID: {user.get('ID')}, Name: {user.get('Name')}, Age: {user.get('Age')}, City: {user.get('City')}\")\n",
        "\n",
        "        # Query 4: Users with 'Python' skill (demonstrating querying embedded arrays)\n",
        "        print(\"\\nQuery 4: Users with 'Python' skill\")\n",
        "        results = list(collection.find({\"Skills\": \"Python\"}))\n",
        "\n",
        "        print(\"\\nUsers with 'Python' skill:\")\n",
        "        for user in results:\n",
        "             print(f\"ID: {user.get('ID')}, Name: {user.get('Name')}, Age: {user.get('Age')}, City: {user.get('City')}, Skills: {user.get('Skills')}\")\n",
        "\n",
        "\n",
        "    except ConnectionFailure as e:\n",
        "        print(f\"✗ MongoDB connection error: Could not connect to MongoDB server. Please ensure MongoDB is running and accessible.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ MongoDB error: {e}\")\n",
        "    finally:\n",
        "        if 'client' in locals() and client:\n",
        "            client.close()\n",
        "            print(f\"\\nMongoDB connection closed.\")\n"
      ],
      "metadata": {
        "id": "Ar-cUiKSdcvP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call sqlite_operations\n",
        "sqlite_operations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvHGhZffbe8o",
        "outputId": "cca72830-9958-4561-c7a4-fbc83f4b061c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Creating SQLite database: users.db\n",
            "2. Creating users table...\n",
            "3. Inserting sample records...\n",
            "✓ Inserted 6 records into SQLite\n",
            "\n",
            "4. Retrieving data using SQL queries:\n",
            "\n",
            "Query 1: All users\n",
            "ID  Name            Age  City         Department   Salary  \n",
            "-----------------------------------------------------------------\n",
            "1   Amara Johnson   25   Beijing      Engineering  75000.0 \n",
            "2   Kwame Smith     30   Shanghai     Marketing    65000.0 \n",
            "3   Zara Brown      28   Guangzhou    Finance      70000.0 \n",
            "4   Kofi Davis      32   Shenzhen     HR           60000.0 \n",
            "5   Asha Wilson     27   Chengdu      Engineering  78000.0 \n",
            "6   Jabari Miller   35   Hangzhou     Sales        72000.0 \n",
            "\n",
            "Query 2: Users older than 29\n",
            "\n",
            "Users older than 29:\n",
            "ID  Name            Age  City         Department   Salary  \n",
            "-----------------------------------------------------------------\n",
            "2   Kwame Smith     30   Shanghai     Marketing    65000.0 \n",
            "4   Kofi Davis      32   Shenzhen     HR           60000.0 \n",
            "6   Jabari Miller   35   Hangzhou     Sales        72000.0 \n",
            "\n",
            "Query 3: Average salary by department\n",
            "\n",
            "Average salary by department:\n",
            "Department      Average Salary \n",
            "------------------------------\n",
            "Engineering     76500.00       \n",
            "Finance         70000.00       \n",
            "HR              60000.00       \n",
            "Marketing       65000.00       \n",
            "Sales           72000.00       \n",
            "\n",
            "SQLite connection closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call MONGODB_operations\n",
        "mongodb_operations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2SNwIF_cOuG",
        "outputId": "3a3c5716-6705-4b03-cd92-835179e44a49"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Connecting to MongoDB...\n",
            "✗ MongoDB connection error: Could not connect to MongoDB server. Please ensure MongoDB is running and accessible.\n",
            "Error details: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 5.0s, Topology Description: <TopologyDescription id: 6872ce9de6f147db4b042fbd, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n",
            "\n",
            "MongoDB connection closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COULD NOT work with mongobd"
      ],
      "metadata": {
        "id": "BeJrzoyBd02E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html  /content/Assignment1_DataEngineeringFundamentals.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPd6zBOrch8A",
        "outputId": "fc51dfe3-9dd4-4692-ae2d-afa1327f2cfb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/Assignment1_DataEngineeringFundamentals.ipynb to html\n",
            "[NbConvertApp] Writing 399439 bytes to /content/Assignment1_DataEngineeringFundamentals.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adMGFst4fGYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}